{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "from presidio_evaluator import ModelEvaluator\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MY_PRESIDIO_ENDPOINT = \"http://localhost:8080/api/v1/projects/test/analyze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Presidio instance via the Presidio API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Read dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 60 samples\n"
     ]
    }
   ],
   "source": [
    "input_samples = read_synth_dataset(\"../data/Synth/synth_test.json\")\n",
    "print(\"Read {} samples\".format(len(input_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'EMAIL': 8,\n",
       "         'LOCATION': 12,\n",
       "         'PERSON': 44,\n",
       "         'BIRTHDAY': 1,\n",
       "         'ORGANIZATION': 5,\n",
       "         'PHONE_NUMBER': 2,\n",
       "         'URL': 1,\n",
       "         'IBAN': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "count_per_entity = Counter([span.entity_type for span in flatten([input_sample.spans for input_sample in input_samples])])\n",
    "count_per_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Match the dataset's entity names with Presidio's entity names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between dataset entities and Presidio entities. Key: Dataset entity, Value: Presidio entity\n",
    "entities_mapping = {\n",
    "    'EMAIL': 'EMAIL_ADDRESS',\n",
    "    'LOCATION':'LOCATION',\n",
    "    'PERSON': 'PERSON',\n",
    "    'BIRTHDAY': 'BIRTHDAY',\n",
    "    'ORGANIZATION':'ORG',\n",
    "    'PHONE_NUMBER': 'PHONE_NUMBER',\n",
    "    'URL': 'DOMAIN_NAME',\n",
    "    'URL': 'URL',\n",
    "    'IBAN': 'IBAN_CODE',\n",
    "    'O': 'O'\n",
    "}\n",
    "\n",
    "new_list = ModelEvaluator.align_input_samples_to_presidio_analyzer(input_samples,\n",
    "                                                                   entities_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Recalculate statistics on updated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'EMAIL_ADDRESS': 8,\n",
       "         'LOCATION': 12,\n",
       "         'PERSON': 44,\n",
       "         'BIRTHDAY': 1,\n",
       "         'ORG': 5,\n",
       "         'PHONE_NUMBER': 2,\n",
       "         'URL': 1,\n",
       "         'IBAN_CODE': 2})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## recheck counter\n",
    "count_per_entity_new = Counter([span.entity_type for span in flatten([input_sample.spans for input_sample in new_list])])\n",
    "count_per_entity_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Run the presidio-evaluator framework with Presidio's API as the 'model' at test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating <class 'presidio_evaluator.presidio_api_evaluator.PresidioAPIEvaluator'>:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model en_core_web_trf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating <class 'presidio_evaluator.presidio_api_evaluator.PresidioAPIEvaluator'>: 100%|██████████| 60/60 [00:15<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from presidio_evaluator import PresidioAPIEvaluator\n",
    "presidio = PresidioAPIEvaluator(all_fields=False, entities_to_keep=list(count_per_entity_new.keys()),endpoint=MY_PRESIDIO_ENDPOINT)\n",
    "evaluted_samples = presidio.evaluate_all(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Extract statistics\n",
    "- Presicion, recall and F measure are calculated based on a PII/Not PII binary classification per token.\n",
    "- Specific entity recall and precision are calculated on the specific PII entity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result = presidio.calculate_score(evaluted_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Entity                     Precision                        Recall\n",
      "                 EMAIL_ADDRESS                       100.00%                       100.00%\n",
      "                      LOCATION                        94.34%                       100.00%\n",
      "                        PERSON                        81.01%                        95.52%\n",
      "                      BIRTHDAY                       100.00%                       100.00%\n",
      "                           ORG                        88.89%                       100.00%\n",
      "                  PHONE_NUMBER                          nan%                         0.00%\n",
      "                           URL                       100.00%                       100.00%\n",
      "                     IBAN_CODE                       100.00%                       100.00%\n",
      "                           PII                        88.89%                        95.36%\n",
      "PII F measure: 0.9201277955271565\n"
     ]
    }
   ],
   "source": [
    "evaluation_result.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. Analyze wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = evaluation_result.model_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common false positive tokens:\n",
      "[]\n",
      "Example sentence with each FP token:\n"
     ]
    }
   ],
   "source": [
    "ModelEvaluator.most_common_fp_tokens(errors,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors of type FP and entity PERSON were found\n"
     ]
    }
   ],
   "source": [
    "fps_df = ModelEvaluator.get_fps_dataframe(errors,entity='PERSON')\n",
    "fps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>prediction</th>\n",
       "      <th>token</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NameSet</th>\n",
       "      <th>Country</th>\n",
       "      <th>Lowercase</th>\n",
       "      <th>Template#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>O</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>For my take on Mr. Wilson, see Guilty Pleasure...</td>\n",
       "      <td>male</td>\n",
       "      <td>American</td>\n",
       "      <td>Latvia</td>\n",
       "      <td>False</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wrong entity</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Spartacus</td>\n",
       "      <td>Spartacus is a very sympathetic person. He's a...</td>\n",
       "      <td>male</td>\n",
       "      <td>Russian</td>\n",
       "      <td>Malta</td>\n",
       "      <td>False</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>O</td>\n",
       "      <td>Souza</td>\n",
       "      <td>Unlike the Souza novel, it's not about necroph...</td>\n",
       "      <td>female</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Mauritania</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wrong entity</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>Raisová</td>\n",
       "      <td>Unlike the Raisová novel, it's not about necro...</td>\n",
       "      <td>female</td>\n",
       "      <td>Czech</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     error_type annotation prediction      token  \\\n",
       "0            FN     PERSON          O     Wilson   \n",
       "1  Wrong entity     PERSON        ORG  Spartacus   \n",
       "2            FN     PERSON          O      Souza   \n",
       "3  Wrong entity     PERSON   LOCATION    Raisová   \n",
       "\n",
       "                                           full_text  Gender   NameSet  \\\n",
       "0  For my take on Mr. Wilson, see Guilty Pleasure...    male  American   \n",
       "1  Spartacus is a very sympathetic person. He's a...    male   Russian   \n",
       "2  Unlike the Souza novel, it's not about necroph...  female    Brazil   \n",
       "3  Unlike the Raisová novel, it's not about necro...  female     Czech   \n",
       "\n",
       "      Country  Lowercase  Template#  \n",
       "0      Latvia      False         95  \n",
       "1       Malta      False         87  \n",
       "2  Mauritania      False         96  \n",
       "3     Belgium      False         96  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fns_df = ModelEvaluator.get_fns_dataframe(errors,entity='PERSON')\n",
    "fns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(input_samples):\n",
    "    if 'Kristian shouted at Enrico' in sample.full_text:\n",
    "        s = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Full text: Kristian shouted at Enrico: \"What are you doing here?\"\n",
       "Spans: [Type: PERSON, value: Kristian, start: 0, end: 8, Type: PERSON, value: Enrico, start: 20, end: 26]\n",
       "Tokens: [Kristian, shouted, at, Enrico, :, \", What, are, you, doing, here, ?, \"]\n",
       "Tags: ['U-PERSON', 'O', 'O', 'U-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_samples[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = evaluted_samples[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('PERSON', 'PERSON'): 1, ('O', 'O'): 12})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
