{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook takes train and test  datasets (of type `List[InputSample]`)\n",
    "and transforms them into two structures consumed by Spacy:\n",
    "1. Spacy JSON (see https://spacy.io/api/annotation#json-input)\n",
    "2. Spacy Pickle files (of structure `[(full_text,\"entities\":[(start, end, type),(...))]`.  \n",
    "See more details here: https://spacy.io/api/annotation#json-input)\n",
    "\n",
    "JSON is used for Spacy's CLI trainer. \n",
    "Pickle is used for fine-tuning using the logic in [../models/spacy_retrain.py](../models/spacy_retrain.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_train = read_synth_dataset('../../data/synth_train.json')\n",
    "conll_train = read_synth_dataset('../../data/conll_train.json')\n",
    "ontonotes_train = read_synth_dataset('../../data/ontonotes_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = conll_train + ontonotes_train # + synth_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_val = read_synth_dataset('../../data/synth_val.json')\n",
    "conll_val = read_synth_dataset('../../data/conll_val.json')\n",
    "ontonotes_val = read_synth_dataset('../../data/ontonotes_val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = conll_val + ontonotes_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_test = read_synth_dataset('../../data/synth_test.json')\n",
    "conll_test = read_synth_dataset('../../data/conll_test.json')\n",
    "ontonotes_test = read_synth_dataset('../../data/ontonotes_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = conll_test + ontonotes_test # + synth_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train 13026 samples\n",
      "Read val 4049 samples\n",
      "Read test 3989 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Read train {} samples\".format(len(train_samples)))\n",
    "print(\"Read val {} samples\".format(len(val_samples)))\n",
    "print(\"Read test {} samples\".format(len(test_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For training, keep only sentences with entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 6773 samples after removal of non-tagged samples\n"
     ]
    }
   ],
   "source": [
    "train_tagged = [sample for sample in train_samples if len(sample.spans)>0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples\".format(len(train_tagged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evaluate training set's entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found in training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-LOCATION',\n",
       " 'B-MALE_TITLE',\n",
       " 'B-NATIONALITY',\n",
       " 'B-NATION_MAN',\n",
       " 'B-NATION_PLURAL',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-PERSON',\n",
       " 'I-LOCATION',\n",
       " 'I-NATIONALITY',\n",
       " 'I-NATION_PLURAL',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-PERSON',\n",
       " 'L-LOCATION',\n",
       " 'L-MALE_TITLE',\n",
       " 'L-NATIONALITY',\n",
       " 'L-NATION_MAN',\n",
       " 'L-NATION_PLURAL',\n",
       " 'L-ORGANIZATION',\n",
       " 'L-PERSON',\n",
       " 'O',\n",
       " 'U-FEMALE_TITLE',\n",
       " 'U-LOCATION',\n",
       " 'U-MALE_TITLE',\n",
       " 'U-NATIONALITY',\n",
       " 'U-NATION_MAN',\n",
       " 'U-NATION_PLURAL',\n",
       " 'U-ORGANIZATION',\n",
       " 'U-PERSON'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Entities found in training set:\")\n",
    "entities = []\n",
    "for sample in train_tagged:\n",
    "    entities.extend([tag for tag in sample.tags])\n",
    "set(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Spacy dataset (option 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator import InputSample\n",
    "import pickle\n",
    "\n",
    "spacy_train = InputSample.create_spacy_dataset(train_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPE', 'O', 'ORG', 'PERSON'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_spacy = [x[1]['entities'] for x in spacy_train]\n",
    "entities_spacy\n",
    "entities_spacy_flat = []\n",
    "for samp in entities_spacy:\n",
    "    for ent in samp:\n",
    "        entities_spacy_flat.append(ent[2])\n",
    "set(entities_spacy_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spacy dataset (option 1: JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6773it [00:00, 31822.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from presidio_evaluator import InputSample\n",
    "spacy_train_json = InputSample.create_spacy_json(train_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick evaluation of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': [{'orth': 'The', 'tag': 'DT', 'ner': 'O'},\n",
       "   {'orth': 'Romanian', 'tag': 'NNP', 'ner': 'U-GPE'},\n",
       "   {'orth': 'found', 'tag': 'VBD', 'ner': 'O'},\n",
       "   {'orth': 'the', 'tag': 'DT', 'ner': 'O'},\n",
       "   {'orth': 'mark', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'again', 'tag': 'RB', 'ner': 'O'},\n",
       "   {'orth': 'two', 'tag': 'CD', 'ner': 'O'},\n",
       "   {'orth': 'minutes', 'tag': 'NNS', 'ner': 'O'},\n",
       "   {'orth': 'after', 'tag': 'IN', 'ner': 'O'},\n",
       "   {'orth': 'halftime', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'and', 'tag': 'CC', 'ner': 'O'},\n",
       "   {'orth': 'again', 'tag': 'RB', 'ner': 'O'},\n",
       "   {'orth': 'in', 'tag': 'IN', 'ner': 'O'},\n",
       "   {'orth': 'the', 'tag': 'DT', 'ner': 'O'},\n",
       "   {'orth': '56th', 'tag': 'JJ', 'ner': 'O'},\n",
       "   {'orth': 'minute', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'before', 'tag': 'IN', 'ner': 'O'},\n",
       "   {'orth': 'midfielder', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'Mathias', 'tag': 'NNP', 'ner': 'B-PERSON'},\n",
       "   {'orth': 'Jespersen', 'tag': 'NNP', 'ner': 'L-PERSON'},\n",
       "   {'orth': 'scored', 'tag': 'VBD', 'ner': 'O'},\n",
       "   {'orth': 'a', 'tag': 'DT', 'ner': 'O'},\n",
       "   {'orth': 'consolation', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'goal', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': 'for', 'tag': 'IN', 'ner': 'O'},\n",
       "   {'orth': 'Herman', 'tag': 'NNP', 'ner': 'B-ORG'},\n",
       "   {'orth': \"'S\", 'tag': 'POS', 'ner': 'I-ORG'},\n",
       "   {'orth': 'World', 'tag': 'NNP', 'ner': 'I-ORG'},\n",
       "   {'orth': 'Of', 'tag': 'IN', 'ner': 'I-ORG'},\n",
       "   {'orth': 'Sporting', 'tag': 'NNP', 'ner': 'I-ORG'},\n",
       "   {'orth': 'Goods', 'tag': 'NNS', 'ner': 'L-ORG'},\n",
       "   {'orth': 'five', 'tag': 'CD', 'ner': 'O'},\n",
       "   {'orth': 'minutes', 'tag': 'NNS', 'ner': 'O'},\n",
       "   {'orth': 'from', 'tag': 'IN', 'ner': 'O'},\n",
       "   {'orth': 'time', 'tag': 'NN', 'ner': 'O'},\n",
       "   {'orth': '.', 'tag': '.', 'ner': 'O'}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_train_json[0]['paragraphs'][0]['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump training set to pickle and json respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "with open(\"../../data/spacy_train.pickle\", 'wb') as handle:\n",
    "    pickle.dump(spacy_train,handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"../../data/spacy_train.json\",\"w\") as f:\n",
    "    json.dump(spacy_train_json,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create JSON and pickle files for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3989it [00:00, 48871.27it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_test = InputSample.create_spacy_dataset(test_samples)\n",
    "spacy_test_json = InputSample.create_spacy_json(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump test set to pickle and json respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../data/spacy_test.pickle\", 'wb') as handle:\n",
    "    pickle.dump(spacy_test,handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"../../data/spacy_test.json\",\"w\") as f:\n",
    "    json.dump(spacy_test_json,f)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4049it [00:00, 42553.55it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_val = InputSample.create_spacy_dataset(val_samples)\n",
    "spacy_val_json = InputSample.create_spacy_json(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../data/spacy_val.pickle\", 'wb') as handle:\n",
    "    pickle.dump(spacy_val,handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"../../data/spacy_val.json\",\"w\") as f:\n",
    "    json.dump(spacy_val_json,f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
